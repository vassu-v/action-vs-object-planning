{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Organism: A Non-Linguistic Cognitive Agent in an OS-Like Environment\n",
        "\n",
        "This notebook implements:\n",
        "- a filesystem-like environment\n",
        "- a reference cognitive agent architecture\n",
        "- imagination-based planning with goal conditioning\n",
        "\n",
        "The focus is on **grounded, non-linguistic cognition**, not language or tool calling.\n",
        "\n",
        "\n",
        "The notebook is organized top-down:\n",
        "\n",
        "environment â†’ perception â†’ models â†’ planning â†’ goals â†’ evaluation."
      ],
      "metadata": {
        "id": "i76OBLYkr_Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment"
      ],
      "metadata": {
        "id": "zZZCn7RcIKmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"creature_world\")\n",
        "\n",
        "DIRS = [\"docs\", \"scripts\", \"data\", \"logs\", \"tmp\"]\n",
        "\n",
        "FILES = {\n",
        "    \"docs/notes.txt\": \"meeting at 3pm\\n\",\n",
        "    \"docs/todo.txt\": \"- buy milk\\n- finish project\\n\",\n",
        "    \"scripts/sum.py\": \"nums = [1, 2, 3, 4]\\nprint(sum(nums))\\n\",\n",
        "    \"data/numbers.txt\": \"1,2,3,4\\n\"\n",
        "}\n",
        "\n",
        "def create_world():\n",
        "    if ROOT.exists():\n",
        "        print(\"World already exists\")\n",
        "        return\n",
        "\n",
        "    ROOT.mkdir()\n",
        "    for d in DIRS:\n",
        "        (ROOT / d).mkdir()\n",
        "\n",
        "    for path, content in FILES.items():\n",
        "        (ROOT / path).write_text(content)\n",
        "\n",
        "    print(\"Creature world created.\")\n",
        "\n",
        "create_world()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJwlOSXtsBhb",
        "outputId": "01a07d0f-b7da-4b00-bc9e-e4a279f5d3c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creature world created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perception"
      ],
      "metadata": {
        "id": "iRighL2GISi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "BASE_DIM = 64\n",
        "OBJECT_DIM = 128\n",
        "\n",
        "def hash_to_vec(text: str, dim=BASE_DIM):\n",
        "    h = hashlib.sha256(text.encode()).digest()\n",
        "    nums = np.frombuffer(h, dtype=np.uint8).astype(np.float32)\n",
        "    vec = np.tile(nums, dim // len(nums) + 1)[:dim]\n",
        "    return vec / 255.0\n",
        "\n",
        "def project(vec, out_dim=OBJECT_DIM):\n",
        "    if len(vec) > out_dim:\n",
        "        return vec[:out_dim]\n",
        "    elif len(vec) < out_dim:\n",
        "        return np.concatenate([vec, np.zeros(out_dim - len(vec))])\n",
        "    return vec\n",
        "\n",
        "def encode_file(path: Path):\n",
        "    content = path.read_text()\n",
        "    meta = f\"FILE|{path.suffix}|{path.parent.name}|{len(content)}\"\n",
        "    return project(np.concatenate([\n",
        "        hash_to_vec(content),\n",
        "        hash_to_vec(meta)\n",
        "    ]))\n",
        "\n",
        "def encode_dir(path: Path):\n",
        "    meta = f\"DIR|{path.name}|{len(list(path.iterdir()))}\"\n",
        "    return project(hash_to_vec(meta))\n",
        "\n",
        "def encode_world(root: Path):\n",
        "    objects = {}\n",
        "    parts = []\n",
        "\n",
        "    for item in sorted(root.rglob(\"*\")):\n",
        "        if item.is_dir():\n",
        "            vec = encode_dir(item)\n",
        "            key = f\"dir:{item.relative_to(root)}\"\n",
        "        else:\n",
        "            vec = encode_file(item)\n",
        "            key = f\"file:{item.relative_to(root)}\"\n",
        "\n",
        "        objects[key] = vec\n",
        "        parts.append(vec)\n",
        "\n",
        "    global_vec = np.mean(np.stack(parts), axis=0)\n",
        "\n",
        "    return objects, global_vec\n",
        "\n",
        "objects, global_vec = encode_world(ROOT)\n",
        "print(len(objects), global_vec.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06y8f1Fmsc69",
        "outputId": "9003f6a7-b9fb-49e4-c223-2ae1c9b0fb0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 (128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def read_file(path: Path):\n",
        "    return path.read_text()\n",
        "\n",
        "def write_file(path: Path, text: str):\n",
        "    path.write_text(path.read_text() + text)\n",
        "\n",
        "def replace_file(path: Path, text: str):\n",
        "    path.write_text(text)\n",
        "\n",
        "def create_file(dir_path: Path, name: str, text: str = \"\"):\n",
        "    file_path = dir_path / name\n",
        "    if file_path.exists():\n",
        "        raise RuntimeError(\"File already exists\")\n",
        "    file_path.write_text(text)\n",
        "\n",
        "def move_file(src: Path, dst_dir: Path):\n",
        "    src.rename(dst_dir / src.name)\n",
        "\n",
        "def run_file(path: Path):\n",
        "    result = subprocess.run(\n",
        "        [\"python3\", str(path)],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        cwd=ROOT\n",
        "    )\n",
        "    log_path = ROOT / \"logs\" / f\"{path.stem}.log\"\n",
        "    log_path.write_text(result.stdout.strip())\n"
      ],
      "metadata": {
        "id": "dlA6Bv8dIYpC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Action:\n",
        "    type: str\n",
        "    args: tuple\n",
        "\n",
        "def apply_action(action: Action):\n",
        "    t, a = action.type, action.args\n",
        "\n",
        "    if t == \"READ\":\n",
        "        return read_file(ROOT / a[0])\n",
        "\n",
        "    elif t == \"WRITE\":\n",
        "        write_file(ROOT / a[0], a[1])\n",
        "\n",
        "    elif t == \"REPLACE\":\n",
        "        replace_file(ROOT / a[0], a[1])\n",
        "\n",
        "    elif t == \"CREATE\":\n",
        "        create_file(ROOT / a[0], a[1], a[2])\n",
        "\n",
        "    elif t == \"MOVE\":\n",
        "        move_file(ROOT / a[0], ROOT / a[1])\n",
        "\n",
        "    elif t == \"RUN\":\n",
        "        run_file(ROOT / a[0])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown action\")\n"
      ],
      "metadata": {
        "id": "W3hRvZEuIa3k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experience & Dataset Generation"
      ],
      "metadata": {
        "id": "rgAkLavAIiuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def reset_world():\n",
        "    if ROOT.exists():\n",
        "        shutil.rmtree(ROOT)\n",
        "    create_world()\n"
      ],
      "metadata": {
        "id": "P0ucGd_zJUQ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def random_text(n=20):\n",
        "    return ''.join(random.choice(string.ascii_lowercase + \" \") for _ in range(n))\n",
        "\n",
        "def traj_append_random():\n",
        "    file_path = random.choice([\n",
        "        \"docs/notes.txt\",\n",
        "        \"docs/todo.txt\"\n",
        "    ])\n",
        "    return [\n",
        "        Action(\"WRITE\", (file_path, \"\\n\" + random_text()))\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "RtpvWpcQK374"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traj_run_and_store():\n",
        "    report_name = f\"report_{random.randint(0,999)}.txt\"\n",
        "    return [\n",
        "        Action(\"RUN\", (\"scripts/sum.py\",)),\n",
        "        Action(\"CREATE\", (\"docs\", report_name, \"Result:\\n\")),\n",
        "        Action(\"WRITE\", (f\"docs/{report_name}\", \"See logs/sum.log\\n\"))\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "GXAK3EuXK6CJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traj_make_summary():\n",
        "    notes = (ROOT / \"docs/notes.txt\").read_text()\n",
        "    todo = (ROOT / \"docs/todo.txt\").read_text()\n",
        "\n",
        "    summary_name = f\"summary_{random.randint(0,999)}.txt\"\n",
        "\n",
        "    return [\n",
        "        Action(\"CREATE\", (\"docs\", summary_name, \"SUMMARY\\n\")),\n",
        "        Action(\"WRITE\", (f\"docs/{summary_name}\", notes)),\n",
        "        Action(\"WRITE\", (f\"docs/{summary_name}\", \"\\n\")),\n",
        "        Action(\"WRITE\", (f\"docs/{summary_name}\", todo)),\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "u-ykxS09K7-o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def snapshot_world():\n",
        "    \"\"\"\n",
        "    Capture the creature's full perception of the world.\n",
        "    \"\"\"\n",
        "    objects, global_vec = encode_world(ROOT)\n",
        "    return {\n",
        "        \"objects\": objects,\n",
        "        \"global\": global_vec\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9miMC_dTLMmt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout_trajectory(action_sequence):\n",
        "    data = []\n",
        "\n",
        "    for action in action_sequence:\n",
        "        before = snapshot_world()\n",
        "        apply_action(action)\n",
        "        after = snapshot_world()\n",
        "\n",
        "        data.append({\n",
        "            \"state_before\": before,\n",
        "            \"action\": action,\n",
        "            \"state_after\": after\n",
        "        })\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "0TAaZDbNK-JN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n=100):\n",
        "    dataset = []\n",
        "\n",
        "    generators = [\n",
        "        traj_append_random,\n",
        "        traj_run_and_store,\n",
        "        traj_make_summary\n",
        "    ]\n",
        "\n",
        "    for _ in range(n):\n",
        "        reset_world()\n",
        "        traj_fn = random.choice(generators)\n",
        "        actions = traj_fn()\n",
        "        dataset.extend(rollout_trajectory(actions))\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "E1lmy_eRK_MK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = generate_dataset(200)\n",
        "len(dataset)\n"
      ],
      "metadata": {
        "id": "oIpjb0NXKPuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d1f2b0-9bbd-4a65-b1f1-93be62a19ef3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n",
            "Creature world created.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "526"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Planning"
      ],
      "metadata": {
        "id": "S8kQgXlgJ1FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---- Action vocabulary (LOCKED) ----\n",
        "ACTION_TYPES = [\"READ\", \"WRITE\", \"REPLACE\", \"CREATE\", \"MOVE\", \"RUN\"]\n",
        "ACTION_TO_ID = {a: i for i, a in enumerate(ACTION_TYPES)}\n",
        "\n",
        "def encode_action(action):\n",
        "    \"\"\"\n",
        "    Symbolic action encoding:\n",
        "    - one-hot for action type\n",
        "    - simple numeric features for args length\n",
        "    \"\"\"\n",
        "    one_hot = np.zeros(len(ACTION_TYPES), dtype=np.float32)\n",
        "    one_hot[ACTION_TO_ID[action.type]] = 1.0\n",
        "\n",
        "    # minimal, cheap arg features (do NOT overthink)\n",
        "    arg_feats = np.array([\n",
        "        len(action.args),\n",
        "        sum(len(str(a)) for a in action.args)\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    return np.concatenate([one_hot, arg_feats])\n",
        "\n",
        "\n",
        "def build_training_pairs(dataset):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for sample in dataset:\n",
        "        s = sample[\"state_before\"][\"global\"]\n",
        "        a = encode_action(sample[\"action\"])\n",
        "        s_next = sample[\"state_after\"][\"global\"]\n",
        "\n",
        "        X.append(np.concatenate([s, a]))\n",
        "        Y.append(s_next)\n",
        "\n",
        "    return np.stack(X), np.stack(Y)\n",
        "\n",
        "\n",
        "X, Y = build_training_pairs(dataset)\n",
        "print(X.shape, Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUygQbKO7fMx",
        "outputId": "659a846d-c6a9-4ed4-8d88-f5416b926b7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(526, 136) (526, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "output_dim = Y.shape[1]\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "model = WorldModel().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "87ytm6jTL2VK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors\n",
        "Xt = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
        "Yt = torch.tensor(Y, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = torch.randperm(len(Xt))\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, len(Xt), batch_size):\n",
        "        idx = perm[i:i+batch_size]\n",
        "        xb, yb = Xt[idx], Yt[idx]\n",
        "\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(xb)\n",
        "\n",
        "    avg_loss = total_loss / len(Xt)\n",
        "    print(f\"Epoch {epoch:02d} | loss = {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "t6_ZcRdoMJMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597a9487-00ad-4969-8911-eb6b7c87f2b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00 | loss = 0.090259\n",
            "Epoch 01 | loss = 0.018719\n",
            "Epoch 02 | loss = 0.009035\n",
            "Epoch 03 | loss = 0.003714\n",
            "Epoch 04 | loss = 0.002105\n",
            "Epoch 05 | loss = 0.001615\n",
            "Epoch 06 | loss = 0.001351\n",
            "Epoch 07 | loss = 0.001230\n",
            "Epoch 08 | loss = 0.001163\n",
            "Epoch 09 | loss = 0.001125\n",
            "Epoch 10 | loss = 0.001100\n",
            "Epoch 11 | loss = 0.001085\n",
            "Epoch 12 | loss = 0.001060\n",
            "Epoch 13 | loss = 0.001044\n",
            "Epoch 14 | loss = 0.001025\n",
            "Epoch 15 | loss = 0.001015\n",
            "Epoch 16 | loss = 0.001008\n",
            "Epoch 17 | loss = 0.000987\n",
            "Epoch 18 | loss = 0.000976\n",
            "Epoch 19 | loss = 0.000962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    i = np.random.randint(len(X))\n",
        "    x = Xt[i:i+1]\n",
        "    y_true = Yt[i]\n",
        "    y_pred = model(x)[0]\n",
        "\n",
        "    mse = torch.mean((y_true - y_pred) ** 2).item()\n",
        "    cos = torch.nn.functional.cosine_similarity(y_true, y_pred, dim=0).item()\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Cosine similarity:\", cos)\n"
      ],
      "metadata": {
        "id": "ohkRQLipMPkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0724b3-bb0e-4484-815a-98f1f077780b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.0025178391952067614\n",
            "Cosine similarity: 0.9917025566101074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build (state -> action_type) pairs\n",
        "def build_policy_data(dataset):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for sample in dataset:\n",
        "        state = sample[\"state_before\"][\"global\"]\n",
        "        action = sample[\"action\"]\n",
        "\n",
        "        X.append(state)\n",
        "        Y.append(ACTION_TO_ID[action.type])\n",
        "\n",
        "    return np.stack(X), np.array(Y)\n",
        "\n",
        "\n",
        "Xp, Yp = build_policy_data(dataset)\n",
        "print(Xp.shape, Yp.shape)\n"
      ],
      "metadata": {
        "id": "Uz9wdJfSM8Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b386470f-87bb-4f79-df29-b1316fa913d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(526, 128) (526,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(ACTION_TYPES))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "policy = PolicyNet().to(DEVICE)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Y0rBUR0lM-f4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xp_t = torch.tensor(Xp, dtype=torch.float32).to(DEVICE)\n",
        "Yp_t = torch.tensor(Yp, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = torch.randperm(len(Xp_t))\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, len(Xp_t), batch_size):\n",
        "        idx = perm[i:i+batch_size]\n",
        "        xb, yb = Xp_t[idx], Yp_t[idx]\n",
        "\n",
        "        logits = policy(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(xb)\n",
        "        correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(Xp_t)\n",
        "    acc = correct / len(Xp_t)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | loss = {avg_loss:.4f} | acc = {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "T1A0h_17NAVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63fd2fb1-732e-4a71-8ec8-175b687db83b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00 | loss = 1.6859 | acc = 0.392\n",
            "Epoch 01 | loss = 1.3397 | acc = 0.608\n",
            "Epoch 02 | loss = 1.1150 | acc = 0.608\n",
            "Epoch 03 | loss = 1.0103 | acc = 0.608\n",
            "Epoch 04 | loss = 0.9642 | acc = 0.608\n",
            "Epoch 05 | loss = 0.9393 | acc = 0.608\n",
            "Epoch 06 | loss = 0.9301 | acc = 0.608\n",
            "Epoch 07 | loss = 0.9259 | acc = 0.608\n",
            "Epoch 08 | loss = 0.9206 | acc = 0.608\n",
            "Epoch 09 | loss = 0.9193 | acc = 0.608\n",
            "Epoch 10 | loss = 0.9155 | acc = 0.608\n",
            "Epoch 11 | loss = 0.9151 | acc = 0.608\n",
            "Epoch 12 | loss = 0.9123 | acc = 0.608\n",
            "Epoch 13 | loss = 0.9108 | acc = 0.608\n",
            "Epoch 14 | loss = 0.9087 | acc = 0.608\n",
            "Epoch 15 | loss = 0.9097 | acc = 0.608\n",
            "Epoch 16 | loss = 0.9054 | acc = 0.608\n",
            "Epoch 17 | loss = 0.9045 | acc = 0.608\n",
            "Epoch 18 | loss = 0.9002 | acc = 0.608\n",
            "Epoch 19 | loss = 0.9033 | acc = 0.608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def index_world_objects(snapshot):\n",
        "    \"\"\"\n",
        "    Assign stable indices to files and directories in a snapshot.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    dirs = []\n",
        "\n",
        "    for k in snapshot[\"objects\"].keys():\n",
        "        if k.startswith(\"file:\"):\n",
        "            files.append(k.replace(\"file:\", \"\"))\n",
        "        elif k.startswith(\"dir:\"):\n",
        "            dirs.append(k.replace(\"dir:\", \"\"))\n",
        "\n",
        "    files = sorted(files)\n",
        "    dirs = sorted(dirs)\n",
        "\n",
        "    file_to_id = {f: i for i, f in enumerate(files)}\n",
        "    dir_to_id = {d: i for i, d in enumerate(dirs)}\n",
        "\n",
        "    return file_to_id, dir_to_id\n"
      ],
      "metadata": {
        "id": "HvdonA_yN0YC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_argument_data(dataset):\n",
        "    file_X, file_Y = [], []\n",
        "    dir_X, dir_Y = [], []\n",
        "\n",
        "    for sample in dataset:\n",
        "        state = sample[\"state_before\"][\"global\"]\n",
        "        action = sample[\"action\"]\n",
        "\n",
        "        snapshot = sample[\"state_before\"]\n",
        "        file_to_id, dir_to_id = index_world_objects(snapshot)\n",
        "\n",
        "        # Actions involving files\n",
        "        if action.type in [\"READ\", \"WRITE\", \"REPLACE\", \"RUN\", \"MOVE\"]:\n",
        "            file_path = action.args[0]\n",
        "            if file_path in file_to_id:\n",
        "                file_X.append(state)\n",
        "                file_Y.append(file_to_id[file_path])\n",
        "\n",
        "        # Actions involving directories\n",
        "        if action.type in [\"CREATE\", \"MOVE\"]:\n",
        "            dir_path = action.args[0] if action.type == \"CREATE\" else action.args[1]\n",
        "            if dir_path in dir_to_id:\n",
        "                dir_X.append(state)\n",
        "                dir_Y.append(dir_to_id[dir_path])\n",
        "\n",
        "    return (\n",
        "        np.stack(file_X), np.array(file_Y),\n",
        "        np.stack(dir_X), np.array(dir_Y)\n",
        "    )\n",
        "\n",
        "\n",
        "file_X, file_Y, dir_X, dir_Y = build_argument_data(dataset)\n",
        "print(file_X.shape, dir_X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRAlGPv1Nfbx",
        "outputId": "d31368c3-72eb-4235-8954-7d2108d5cea3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(393, 128) (133, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FileArgNet(nn.Module):\n",
        "    def __init__(self, max_files):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, max_files)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class DirArgNet(nn.Module):\n",
        "    def __init__(self, max_dirs):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, max_dirs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "4WDJjPDTOT2v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine maximum sizes seen\n",
        "MAX_FILES = int(file_Y.max()) + 1\n",
        "MAX_DIRS = int(dir_Y.max()) + 1\n",
        "\n",
        "file_net = FileArgNet(MAX_FILES).to(DEVICE)\n",
        "dir_net = DirArgNet(MAX_DIRS).to(DEVICE)\n",
        "\n",
        "opt_file = optim.Adam(file_net.parameters(), lr=1e-3)\n",
        "opt_dir = optim.Adam(dir_net.parameters(), lr=1e-3)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# tensors\n",
        "file_Xt = torch.tensor(file_X, dtype=torch.float32).to(DEVICE)\n",
        "file_Yt = torch.tensor(file_Y, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "dir_Xt = torch.tensor(dir_X, dtype=torch.float32).to(DEVICE)\n",
        "dir_Yt = torch.tensor(dir_Y, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# train file selector\n",
        "for epoch in range(10):\n",
        "    logits = file_net(file_Xt)\n",
        "    loss = loss_fn(logits, file_Yt)\n",
        "\n",
        "    opt_file.zero_grad()\n",
        "    loss.backward()\n",
        "    opt_file.step()\n",
        "\n",
        "    acc = (logits.argmax(1) == file_Yt).float().mean().item()\n",
        "    print(f\"[File] epoch {epoch} | loss {loss.item():.4f} | acc {acc:.3f}\")\n",
        "\n",
        "# train dir selector\n",
        "for epoch in range(10):\n",
        "    logits = dir_net(dir_Xt)\n",
        "    loss = loss_fn(logits, dir_Yt)\n",
        "\n",
        "    opt_dir.zero_grad()\n",
        "    loss.backward()\n",
        "    opt_dir.step()\n",
        "\n",
        "    acc = (logits.argmax(1) == dir_Yt).float().mean().item()\n",
        "    print(f\"[Dir ] epoch {epoch} | loss {loss.item():.4f} | acc {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb0ELx7SOWXK",
        "outputId": "76003534-a261-4236-de15-18cc3b2d9325"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[File] epoch 0 | loss 1.3227 | acc 0.186\n",
            "[File] epoch 1 | loss 1.2401 | acc 0.728\n",
            "[File] epoch 2 | loss 1.1686 | acc 0.728\n",
            "[File] epoch 3 | loss 1.1066 | acc 0.728\n",
            "[File] epoch 4 | loss 1.0530 | acc 0.728\n",
            "[File] epoch 5 | loss 1.0074 | acc 0.728\n",
            "[File] epoch 6 | loss 0.9671 | acc 0.728\n",
            "[File] epoch 7 | loss 0.9317 | acc 0.728\n",
            "[File] epoch 8 | loss 0.9018 | acc 0.728\n",
            "[File] epoch 9 | loss 0.8775 | acc 0.728\n",
            "[Dir ] epoch 0 | loss 0.7122 | acc 0.000\n",
            "[Dir ] epoch 1 | loss 0.6337 | acc 1.000\n",
            "[Dir ] epoch 2 | loss 0.5668 | acc 1.000\n",
            "[Dir ] epoch 3 | loss 0.5088 | acc 1.000\n",
            "[Dir ] epoch 4 | loss 0.4618 | acc 1.000\n",
            "[Dir ] epoch 5 | loss 0.4207 | acc 1.000\n",
            "[Dir ] epoch 6 | loss 0.3842 | acc 1.000\n",
            "[Dir ] epoch 7 | loss 0.3522 | acc 1.000\n",
            "[Dir ] epoch 8 | loss 0.3240 | acc 1.000\n",
            "[Dir ] epoch 9 | loss 0.2976 | acc 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.random.randint(len(file_X))\n",
        "state = torch.tensor(file_X[i:i+1], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_id = file_net(state).argmax(dim=1).item()\n",
        "\n",
        "print(\"Predicted file id:\", pred_id)\n",
        "print(\"True file id:\", file_Y[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gozuLnQOtQS",
        "outputId": "a4e702bc-22a8-49a3-9e7a-fa7a9b549cf5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted file id: 2\n",
            "True file id: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuhU3q3r7Bl_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changes for oe"
      ],
      "metadata": {
        "id": "JB27SCZMdwgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_object_deltas(before, after, eps=1e-3):\n",
        "    \"\"\"\n",
        "    Returns a dict: object_key -> 0/1 indicating change.\n",
        "    \"\"\"\n",
        "    deltas = {}\n",
        "\n",
        "    before_objs = before[\"objects\"]\n",
        "    after_objs = after[\"objects\"]\n",
        "\n",
        "    all_keys = set(before_objs) | set(after_objs)\n",
        "\n",
        "    for k in all_keys:\n",
        "        if k not in before_objs:\n",
        "            deltas[k] = 1  # appeared\n",
        "        elif k not in after_objs:\n",
        "            deltas[k] = 1  # disappeared\n",
        "        else:\n",
        "            diff = np.linalg.norm(after_objs[k] - before_objs[k])\n",
        "            deltas[k] = int(diff > eps)\n",
        "\n",
        "    return deltas\n"
      ],
      "metadata": {
        "id": "w-EBbHQNd1QN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_object_index(dataset):\n",
        "    keys = set()\n",
        "    for sample in dataset:\n",
        "        keys |= set(sample[\"state_before\"][\"objects\"].keys())\n",
        "        keys |= set(sample[\"state_after\"][\"objects\"].keys())\n",
        "\n",
        "    keys = sorted(keys)\n",
        "    return {k: i for i, k in enumerate(keys)}\n"
      ],
      "metadata": {
        "id": "3EGLC2emd52Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OBJECT_INDEX = build_object_index(dataset)\n",
        "NUM_OBJECTS = len(OBJECT_INDEX)\n",
        "print(\"Tracked objects:\", NUM_OBJECTS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POAZ2ZAgd88p",
        "outputId": "a406aac2-b601-4893-dd37-d1a80fd7fa34"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracked objects: 139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_object_effect_data(dataset, object_index):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for sample in dataset:\n",
        "        state = sample[\"state_before\"][\"global\"]\n",
        "        action = encode_action(sample[\"action\"])\n",
        "\n",
        "        deltas = compute_object_deltas(\n",
        "            sample[\"state_before\"],\n",
        "            sample[\"state_after\"]\n",
        "        )\n",
        "\n",
        "        y = np.zeros(len(object_index), dtype=np.float32)\n",
        "        for k, v in deltas.items():\n",
        "            if k in object_index:\n",
        "                y[object_index[k]] = v\n",
        "\n",
        "        X.append(np.concatenate([state, action]))\n",
        "        Y.append(y)\n",
        "\n",
        "    return np.stack(X), np.stack(Y)\n",
        "\n",
        "\n",
        "Xe, Ye = build_object_effect_data(dataset, OBJECT_INDEX)\n",
        "print(Xe.shape, Ye.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6AIbNqEeACF",
        "outputId": "929db774-b97a-4496-c128-604d6ced890d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(526, 136) (526, 139)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sub-training"
      ],
      "metadata": {
        "id": "XHKv1dRA6w8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectEffectModel(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "LxR-eJEnd_0G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effect_model = ObjectEffectModel(Xe.shape[1], Ye.shape[1]).to(DEVICE)\n",
        "opt = optim.Adam(effect_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "Xt = torch.tensor(Xe, dtype=torch.float32).to(DEVICE)\n",
        "Yt = torch.tensor(Ye, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "for epoch in range(20):\n",
        "    logits = effect_model(Xt)\n",
        "    loss = loss_fn(logits, Yt)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    print(f\"epoch {epoch:02d} | loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Q_OWIztseYE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187335c6-a944-4bcb-d17a-75848c3da795"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 | loss 0.7193\n",
            "epoch 01 | loss 0.6248\n",
            "epoch 02 | loss 0.5410\n",
            "epoch 03 | loss 0.4666\n",
            "epoch 04 | loss 0.4002\n",
            "epoch 05 | loss 0.3413\n",
            "epoch 06 | loss 0.2895\n",
            "epoch 07 | loss 0.2445\n",
            "epoch 08 | loss 0.2058\n",
            "epoch 09 | loss 0.1732\n",
            "epoch 10 | loss 0.1460\n",
            "epoch 11 | loss 0.1239\n",
            "epoch 12 | loss 0.1062\n",
            "epoch 13 | loss 0.0922\n",
            "epoch 14 | loss 0.0813\n",
            "epoch 15 | loss 0.0730\n",
            "epoch 16 | loss 0.0668\n",
            "epoch 17 | loss 0.0622\n",
            "epoch 18 | loss 0.0588\n",
            "epoch 19 | loss 0.0565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def action_object_mask(action_type, object_index):\n",
        "    \"\"\"\n",
        "    Returns a binary mask over objects indicating which objects\n",
        "    this action is allowed to affect.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(len(object_index), dtype=np.float32)\n",
        "\n",
        "    for obj, i in object_index.items():\n",
        "        if action_type == \"RUN\":\n",
        "            if obj.startswith(\"file:logs/\"):\n",
        "                mask[i] = 1.0\n",
        "\n",
        "        elif action_type == \"CREATE\":\n",
        "            if obj.startswith(\"file:docs/\"):\n",
        "                mask[i] = 1.0\n",
        "\n",
        "        elif action_type == \"WRITE\":\n",
        "            if obj.startswith(\"file:docs/\") or obj.startswith(\"file:data/\"):\n",
        "                mask[i] = 1.0\n",
        "\n",
        "        elif action_type == \"MOVE\":\n",
        "            if obj.startswith(\"file:\"):\n",
        "                mask[i] = 1.0\n",
        "\n",
        "        elif action_type == \"REPLACE\":\n",
        "            if obj.startswith(\"file:\"):\n",
        "                mask[i] = 1.0\n",
        "\n",
        "        elif action_type == \"READ\":\n",
        "            # READ does not change objects\n",
        "            pass\n",
        "\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "K7vyCf98hukT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_object_effects(snapshot, action_type, top_k=3):\n",
        "    dummy = Action(action_type, ())\n",
        "    a_enc = encode_action(dummy)\n",
        "    x = np.concatenate([snapshot[\"global\"], a_enc])\n",
        "\n",
        "    x_t = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = effect_model(x_t)[0]\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "    # apply action mask\n",
        "    mask = action_object_mask(action_type, OBJECT_INDEX)\n",
        "    probs = probs * mask\n",
        "\n",
        "    # ğŸ”‘ TOP-K instead of threshold\n",
        "    top_ids = probs.argsort()[::-1][:top_k]\n",
        "    return {\n",
        "        obj for obj, i in OBJECT_INDEX.items()\n",
        "        if i in top_ids and probs[i] > 0\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2DCCtm_3eam_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OYj8qLZhwML"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLIl69aA8SIp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imagination"
      ],
      "metadata": {
        "id": "dntnQVvlLJ7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_hybrid_transition(before_snap, after_global, action_type, goal):\n",
        "    score = 0.0\n",
        "\n",
        "    # 1. Weak latent signal (keeps imagination relevant)\n",
        "    latent_delta = np.linalg.norm(after_global - before_snap[\"global\"])\n",
        "    score += 0.3 * latent_delta\n",
        "\n",
        "    # 2. Symbolic effects weighted by goal\n",
        "    effects = predict_object_effects(before_snap, action_type)\n",
        "\n",
        "    for e in effects:\n",
        "        if e.startswith(\"file:logs\"):\n",
        "            score += 5.0 * goal.get(\"logs\", 0.0)\n",
        "        if e.startswith(\"file:docs\"):\n",
        "            score += 3.0 * goal.get(\"docs\", 0.0)\n",
        "\n",
        "    # 3. Mild inaction penalty\n",
        "    if action_type == \"READ\":\n",
        "        score -= 1.0\n",
        "\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "Zu7dq4h_TCcg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_priors(snapshot, top_k=3):\n",
        "    state = torch.tensor(\n",
        "        snapshot[\"global\"], dtype=torch.float32\n",
        "    ).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = policy(state)[0]\n",
        "        probs = torch.softmax(logits, dim=0).cpu().numpy()\n",
        "\n",
        "    # top-k action indices\n",
        "    top_ids = probs.argsort()[::-1][:top_k]\n",
        "    return [(ACTION_TYPES[i], probs[i]) for i in top_ids]\n"
      ],
      "metadata": {
        "id": "Vyf2sl6bQX_r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imagine_next(global_state, action_type, file_id=None, dir_id=None):\n",
        "    \"\"\"\n",
        "    Predict next global state embedding using the world model.\n",
        "    \"\"\"\n",
        "    dummy_action = Action(action_type, ())\n",
        "    a_enc = encode_action(dummy_action)\n",
        "\n",
        "    x = np.concatenate([global_state, a_enc])\n",
        "    x_t = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_next = model(x_t)[0].cpu().numpy()\n",
        "\n",
        "    return pred_next\n"
      ],
      "metadata": {
        "id": "OeBz92nSOEtj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_one_step_guided_goal(snapshot, goal, top_k=3):\n",
        "    priors = policy_priors(snapshot, top_k=top_k)\n",
        "\n",
        "    best_score = -1e9\n",
        "    best_action = None\n",
        "\n",
        "    for action_type, prior in priors:\n",
        "        pred_global = imagine_next(snapshot[\"global\"], action_type)\n",
        "\n",
        "        s = score_hybrid_transition(\n",
        "            snapshot, pred_global, action_type, goal\n",
        "        )\n",
        "\n",
        "        # Policy prior biases but does not dominate\n",
        "        s += 0.5 * np.log(prior + 1e-6)\n",
        "\n",
        "        if s > best_score:\n",
        "            best_score = s\n",
        "            best_action = action_type\n",
        "\n",
        "    return best_action, best_score\n"
      ],
      "metadata": {
        "id": "MyP3lUP6Lwsf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal"
      ],
      "metadata": {
        "id": "LNjfjka2Li7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOAL_LOGS = {\n",
        "    \"logs\": 1.0,\n",
        "    \"docs\": 0.0,\n",
        "}\n",
        "\n",
        "GOAL_DOCS = {\n",
        "    \"logs\": 0.0,\n",
        "    \"docs\": 1.0,\n",
        "}\n",
        "\n",
        "GOAL_QUIET = {\n",
        "    \"logs\": -1.0,\n",
        "    \"docs\": 0.0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "rrb5-70DR1Xv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_world()\n",
        "snap = snapshot_world()\n",
        "\n",
        "for name, goal in {\n",
        "    \"LOGS\": GOAL_LOGS,\n",
        "    \"DOCS\": GOAL_DOCS,\n",
        "    \"QUIET\": GOAL_QUIET,\n",
        "}.items():\n",
        "    action, score = plan_one_step_guided_goal(snap, goal)\n",
        "    print(name, \"â†’\", action, \"| score:\", score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVsri_h9THRn",
        "outputId": "128991fb-4322-49a1-8b54-923a9af06b2f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creature world created.\n",
            "LOGS â†’ RUN | score: 4.262936238509378\n",
            "DOCS â†’ CREATE | score: 8.604660490512888\n",
            "QUIET â†’ WRITE | score: -0.008814724999130108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot = snap\n",
        "\n",
        "for action in ACTION_TYPES:\n",
        "    print(action, predict_object_effects(snap, action))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSrcRMiQggEI",
        "outputId": "ed0e8028-1c1d-40b0-90fc-b4ac4df36545"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "READ set()\n",
            "WRITE {'file:docs/report_465.txt', 'file:docs/report_148.txt', 'file:data/numbers.txt'}\n",
            "REPLACE {'file:logs/sum.log', 'file:docs/notes.txt', 'file:data/numbers.txt'}\n",
            "CREATE {'file:docs/report_465.txt', 'file:docs/report_148.txt', 'file:docs/notes.txt'}\n",
            "MOVE {'file:docs/report_148.txt', 'file:logs/sum.log', 'file:data/numbers.txt'}\n",
            "RUN {'file:logs/sum.log'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task-testing"
      ],
      "metadata": {
        "id": "mPs7KkAqv_xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_action_args(action_type, snapshot):\n",
        "    \"\"\"\n",
        "    Minimal argument resolver for task execution.\n",
        "    Uses simple heuristics (not learning yet).\n",
        "    \"\"\"\n",
        "\n",
        "    if action_type == \"RUN\":\n",
        "        return (\"scripts/sum.py\",)\n",
        "\n",
        "    elif action_type == \"READ\":\n",
        "        return (\"docs/notes.txt\",)\n",
        "\n",
        "    elif action_type == \"WRITE\":\n",
        "        return (\"docs/notes.txt\", \"\\nupdate\")\n",
        "\n",
        "    elif action_type == \"REPLACE\":\n",
        "        return (\"docs/notes.txt\", \"replaced content\\n\")\n",
        "\n",
        "    elif action_type == \"CREATE\":\n",
        "        return (\"docs\", f\"task_{np.random.randint(1000)}.txt\", \"task output\\n\")\n",
        "\n",
        "    elif action_type == \"MOVE\":\n",
        "        return (\"docs/notes.txt\", \"tmp\")\n",
        "\n",
        "    else:\n",
        "        return ()\n"
      ],
      "metadata": {
        "id": "_i9tgLcxzq9S"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_task_with_trace(task, max_steps=5, verbose=True):\n",
        "    reset_world()\n",
        "    trace = []\n",
        "\n",
        "    snapshot = snapshot_world()\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Check success BEFORE acting\n",
        "        if task[\"success\"](snapshot):\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"steps\": step,\n",
        "                \"trace\": trace,\n",
        "                \"final_snapshot\": snapshot\n",
        "            }\n",
        "\n",
        "        action_type, score = plan_one_step_guided_goal(\n",
        "            snapshot, task[\"goal\"]\n",
        "        )\n",
        "\n",
        "        args = resolve_action_args(action_type, snapshot)\n",
        "        action = Action(action_type, args)\n",
        "\n",
        "        before = snapshot\n",
        "        apply_action(action)\n",
        "        snapshot = snapshot_world()\n",
        "\n",
        "        # Record trace entry\n",
        "        trace.append({\n",
        "            \"step\": step,\n",
        "            \"action\": action_type,\n",
        "            \"score\": score,\n",
        "            \"objects_before\": set(before[\"objects\"].keys()),\n",
        "            \"objects_after\": set(snapshot[\"objects\"].keys())\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[{step}] ACTION = {action_type:6s} | score = {score:.3f}\")\n",
        "\n",
        "    # Final success check\n",
        "    return {\n",
        "        \"success\": task[\"success\"](snapshot),\n",
        "        \"steps\": max_steps,\n",
        "        \"trace\": trace,\n",
        "        \"final_snapshot\": snapshot\n",
        "    }"
      ],
      "metadata": {
        "id": "-Rje8DIcx3xO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_LOG = {\n",
        "    \"name\": \"Generate Log\",\n",
        "    \"goal\": GOAL_LOGS,\n",
        "    \"success\": lambda snap: \"file:logs/sum.log\" in snap[\"objects\"]\n",
        "}\n",
        "\n",
        "result = run_task_with_trace(TASK_LOG, max_steps=3)\n",
        "print(\"SUCCESS:\", result[\"success\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "749v3J8jx8EL",
        "outputId": "4b4cfb48-6243-4e17-9bdd-a5c6a7e90717"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creature world created.\n",
            "[0] ACTION = RUN    | score = 4.263\n",
            "SUCCESS: True\n"
          ]
        }
      ]
    }
  ]
}